# Hive metastore database configuration.
db:
  # -- Hive metastore driver reference.
  driverRef: org.postgresql.Driver
  # -- Hive metastore driver name.  One of `` `postgresql` or `mysql`
  driverName: postgresql
  # -- Hive metastore database host.
  host:
  # -- Hive metastore database port.
  port: 5432
  # -- Hive metastore database name.
  databaseName: hms
  user:
    # -- Hive metastore database user.
    name: hms
    # Hive metastore database credentials.
    password:
      # -- Hive metastore database existing kubernetes secret name.
      secretName:
      # -- Hive metastore database existing kubernetes secret key containing the password.
      propertyName:

# S3 warehouse configuration
s3:
  # -- S3 endpoint
  url:
  # -- S3 warehouse directory/bucket name.
  warehouseDirectory:
  accessKey:
    # -- S3 access key existing kubernetes secret name.
    secretName:
    # -- S3 access key kubernetes secret key containing the accessKey.
    propertyName:
  secretKey:
    # -- S3 secret key existing kubernetes secret name.
    secretName:
    # -- S3 secret key kubernetes secret key containing the secretName.
    propertyName:
  # -- Proxy configuration (if needed).
  proxy:
    host:
    port:
  requestTimeout: 0

# Cloud storage selection: "s3" or "gcs"
cloud_storage: "s3"

# GCS warehouse configuration
gcs:
  enabled: true
  projectId: ""
  warehouseDirectory: ""
  workloadIdentity:
    enabled: true
    gcpServiceAccount: ""
  serviceAccountKeyFile: ""  # Only used if workloadIdentity.enabled is false

aws:
  # -- AWS Pod execution IAM role.
  podIamRole:
  # -- S3 IAM role ARN for hive-metastore access to S3.
  s3AssumeRoleArn:

# -- Extra environment variables in RAW format that will be passed into pods
extraEnvRaw: []
# Load DB username from other secret
# - name: HIVEMS_USER
#   valueFrom:
#     secretKeyRef:
#       name: my-database-secret
#       key: username
# Set env variable
# - name: MY_ENV_VAR_1
#   value: MY_VALUE_1

# -- Configuration overrides, with env variables support, that will be passed into pods on /etc/hadoop/conf and /etc/hive/conf.
configOverrides: {}
#  Example:
#  core-site.xml:
#    mountPath: /etc/hadoop/conf/core-site.xml
#    stringData: |
#      <?xml version="1.0" encoding="UTF-8"?>
#      <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
#      <configuration>
#        <property>
#          <name>fs.gs.impl</name>
#          <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>
#        </property>
#        <property>
#          <name>fs.AbstractFileSystem.gs.impl</name>
#          <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS</value>
#        </property>
#      </configuration>
#
#  metastore-site.xml:
#    mountPath: /etc/hive/conf/metastore-site.xml
#    stringData: |
#      <?xml version="1.0" encoding="UTF-8" standalone="no"?>
#      <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
#      <configuration>
#        <property>
#          <name>metastore.thrift.uris</name>
#          <value>thrift://localhost:9083</value>
#          <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>
#        </property>
#        <property>
#          <name>metastore.task.threads.always</name>
#          <value>org.apache.hadoop.hive.metastore.events.EventCleanerTask,org.apache.hadoop.hive.metastore.MaterializationsRebuildLockCleanerTask</value>
#        </property>
#        <property>
#          <name>metastore.expression.proxy</name>
#          <value>org.apache.hadoop.hive.metastore.DefaultPartitionExpressionProxy</value>
#        </property>
#        <property>
#          <name>metastore.server.min.threads</name>
#          <value>5</value>
#        </property>
#        <property>
#          <name>metastore.server.max.threads</name>
#          <value>20</value>
#        </property>
#        <property>
#          <name>javax.jdo.option.Multithreaded</name>
#          <value>true</value>
#          <description>Set this to true if multiple threads access metastore through JDO concurrently.</description>
#        </property>
#        <property>
#          <name>javax.jdo.PersistenceManagerFactoryClass</name>
#          <value>org.datanucleus.api.jdo.JDOPersistenceManagerFactory</value>
#          <description>class implementing the jdo persistence</description>
#        </property>
#        <property>
#          <name>javax.jdo.option.ConnectionDriverName</name>
#          <value>${DB_DRIVER_REF}</value>
#        </property>
#        <property>
#          <name>javax.jdo.option.ConnectionURL</name>
#          <value>jdbc:${DB_DRIVER_NAME}://${DB_HOST}:${DB_PORT}/${HIVEMS_DB}</value>
#        </property>
#        <property>
#          <name>javax.jdo.option.ConnectionUserName</name>
#          <value>${HIVEMS_USER}</value>
#        </property>
#        <property>
#          <name>javax.jdo.option.ConnectionPassword</name>
#          <value>${HIVEMS_PASSWORD}</value>
#        </property>
#        <property>
#          <name>fs.s3a.path.style.access</name>
#          <value>true</value>
#        </property>
#        <property>
#          <name>fs.s3a.connection.request.timeout</name>
#          <value>${S3_REQUEST_TIMEOUT}</value>
#        </property>
#        <!-- GCS connector configuration -->
#        <property>
#          <name>fs.gs.impl</name>
#          <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>
#          <description>The FileSystem implementation for gs: URIs</description>
#        </property>
#      </configuration>
#
#  metastore-log4j2.properties:
#    mountPath: /etc/hive/conf/metastore-log4j2.properties
#    stringData: |
#      status = INFO
#      name = MetastoreLog4j2
#      packages = org.apache.hadoop.hive.metastore
#      # list of all appenders
#      appenders = console
#      # console appender
#      appender.console.type = Console
#      appender.console.name = console
#      appender.console.target = SYSTEM_ERR
#      appender.console.layout.type = PatternLayout
#      appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n
#      # list of all loggers
#      loggers = DataNucleus, Datastore, JPOX, PerfLogger
#      logger.DataNucleus.name = DataNucleus
#      logger.DataNucleus.level = ERROR
#      logger.Datastore.name = Datastore
#      logger.Datastore.level = ERROR
#      logger.JPOX.name = JPOX
#      logger.JPOX.level = ERROR
#      logger.PerfLogger.name = org.apache.hadoop.hive.ql.log.PerfLogger
#      logger.PerfLogger.level = INFO
#      # root logger
#      rootLogger.level = ${LOG_LEVEL}
#      rootLogger.appenderRefs = root
#      rootLogger.appenderRef.root.ref = console

# As the metastore does not provide authentication/authorization mechanism,
# everybody will be able to access and modify all metastore data.
# So, we need to restrict access only from allowed namespace
networkPolicies:
  enabled: true
  allowedNamespaceLabels: {}
  allowedNamespace: []

# -- Desired number of hive-metastore pods to run. Set 'replicaCount' to 0 or leave it unused when autoscaling is enabled.
replicaCount: 2

# -- Resource requests and limits for the hive-metastore pod.
resources:
  limits:
    cpu: 500m
    memory: 1024Mi
  requests:
    cpu: 100m
    memory: 512Mi

# -- Enable autoscaling for the hive-metastore component. Disable 'autoscaling' when using the replicaCount parameter.
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 3
  targetCPUUtilizationPercentage:
  targetMemoryUtilizationPercentage:

# -- Log4j2 log level. One of `` `debug`, `info`, `warn`, `error`, `fatal`, `trace`
logLevel: INFO

# -- Hive metastore database initialization job
initJob:
  ttlSecondsAfterFinished: 60
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": "before-hook-creation"

# -- Allow the metastore to be reachable from outside of the cluster
# @raw
#
# ⚠️ As the metastore does not provide authentication/authorization mechanism, everybody will be able
#   to access and modify all metastore data.
#   So use this feature only
#   - Temporary, for test and debugging
#   - If access to your cluster is strictly controlled by some firewall, which will limit access to the
#     exposed address and port.
#
exposure:
  loadbalancer:
    # Only metallb is currently supportcd ed by the helm chart. Contribution welcome.
    metallb:
      enabled: no
      externalIp:
      ipSharingKey:
  nodePort:
    enabled: no
    value:

# Configuration for the Docker image to use.
image:
  # -- Image pullSecrets for private registries.
  pullSecrets:
  - name: data
  # -- Docker image registry.
  repository: quay.io/okdp/hive-metastore
  # -- Image tag.
  tag: latest
  # -- Image pull policy.
  pullPolicy: IfNotPresent

# -- Specifies whether a service account should be created.
serviceAccount:
  create: true
  # Annotations to add to the service account.
  annotations: {}
  # The name of the service account to use.
  name: ""

# -- Security profile for the hive-metastore pod.
podSecurityContext:
  runAsNonRoot: true
  seccompProfile:
    type: RuntimeDefault

containerSecurityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: false
  capabilities:
    drop:
      - "ALL"

# -- Node selector for the hive-metastore pod.
nodeSelector: {}
# -- Tolerations for the hive-metastore pod.
tolerations: []
# -- Affinity for the hive-metastore pod.
affinity: {}

# -- Hive metastore service port.
servicePort: 9083

# -- Annotations to be added to the pod.
podAnnotations: {}

# -- Annotations to be added to all resources.
commonAnnotations: {}


# Following will allow a renaming of all metastore kubernetes resources
# Keep default values in usual cases.
# -- Allow chart name overriding.
nameOverride:
# -- Allow overriding base name of all resources.
fullNameOverride:
# -- Will default to {{ include "metastore.fullname" . }}
deploymentName:
# -- Will default to {{ include "metastore.fullname" . }}
jobName:
# -- Will default to {{ include "metastore.fullname" . }}
serviceName:
# -- Will default to {{ include "metastore.fullname" . }}
networkPolicyName:
# -- Will default to {{ include "metastore.fullname" . }}
hpaName:
